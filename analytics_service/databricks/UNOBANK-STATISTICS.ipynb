{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as f\n",
    "from datetime import datetime, timedelta"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "53ae6024-ad12-4943-9edc-18d64c33aac5"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "KEYSPACE = \"bank\"\n",
    "USERS_DAILY_TABLE = \"unique_users_daily\"\n",
    "TRANSACTIONS_DAILY_TABLE = \"successful_transactions_daily\"\n",
    "OUTPUT_TABLE = \"bank_statistics_daily\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "Define Cassandra Tables",
     "showTitle": true,
     "inputWidgets": {},
     "nuid": "5b9ac636-f21e-4f7a-b05d-944a21e6dafd"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def get_transactions_metrics(date: str) -> (int, int):\n",
    "    \"\"\"\n",
    "    Get the number of trunsactions and capital turnover over the provided day.\n",
    "    Return zeroes in case there are no records.\n",
    "    \n",
    "    :param date: (str) - Date in the format (YYYY-MM-DD).\n",
    "    :return: (int, int) - Number of trunsactions and capital turnover.\n",
    "    \"\"\"\n",
    "    # Get all the transactions for the provided date.\n",
    "    transactions_df = ( \n",
    "        spark.read\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\n",
    "            .options(table=TRANSACTIONS_DAILY_TABLE, keyspace=KEYSPACE)\n",
    "            .load()\n",
    "            .filter(f\"date = '{date}'\")\n",
    "    )\n",
    "    \n",
    "    # Get the total number of transactions and capital turnover over the current day. \n",
    "    transactions_metrics = (\n",
    "        transactions_df\n",
    "            .groupBy(f.col(\"date\"))\n",
    "            .agg(\n",
    "                f.count(f.col(\"transaction_id\")).alias(\"number_transactions\"),\n",
    "                f.sum(f.col(\"amount\")).alias(\"capital_turnover\")\n",
    "            )\n",
    "            .select(f.col(\"number_transactions\"), f.col(\"capital_turnover\"))\n",
    "            .collect()\n",
    "    )\n",
    "    \n",
    "    # In case there were no records for that day, return zeroes.\n",
    "    try:\n",
    "        return transactions_metrics[0].number_transactions, transactions_metrics[0].capital_turnover\n",
    "    except (IndexError, AttributeError):\n",
    "        return 0, 0\n",
    "    \n",
    "    \n",
    "def get_users_metrics(date: str) -> int:\n",
    "    \"\"\"\n",
    "    Get number of unique users for the provided date.\n",
    "    \n",
    "    :param date: (str) - Date in the format (YYYY-MM-DD).\n",
    "    :return: (int) - Number of unique users.\n",
    "    \"\"\"\n",
    "    # Get all the unique users for the provided date.\n",
    "    users_df = ( \n",
    "        spark.read\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\n",
    "            .options(table=USERS_DAILY_TABLE, keyspace=KEYSPACE)\n",
    "            .load()\n",
    "            .filter(f\"date = '{date}'\")\n",
    "            .distinct()\n",
    "    )\n",
    "    # Get the number of unique users.\n",
    "    return users_df.count()\n",
    "\n",
    "\n",
    "def get_bank_statistics(date=None):\n",
    "    \"\"\"\n",
    "    Get bank statistics for the provided date.\n",
    "    If no date was provided, return the maximum statistics \n",
    "    as those are regarded to be last.\n",
    "    Return zeroes in case there are no records.\n",
    "    \n",
    "    :param date: (str) - Date in the format (YYYY-MM-DD).\n",
    "    :return: (int, int, int) - Number of trunsactions, uniques users and capital turnover.\n",
    "    \"\"\"\n",
    "    # Get the statistics.\n",
    "    stats_df = ( \n",
    "        spark.read\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\n",
    "            .options(table=OUTPUT_TABLE, keyspace=KEYSPACE)\n",
    "            .load()\n",
    "    )\n",
    "    \n",
    "    # Collect results for the provided date.\n",
    "    if date:\n",
    "        # Filter by date.\n",
    "        stats = (\n",
    "            stats_df\n",
    "                .filter(f\"date = '{date}'\") \\\n",
    "                .select(\"number_transactions\", \"number_unique_users\", \"capital_turnover\") \\\n",
    "                .collect()\n",
    "        )\n",
    "    else:\n",
    "        # If there is not record for specified date, get the maximum results.\n",
    "        stats = (\n",
    "            stats_df\n",
    "                .select(\n",
    "                    f.max(\"number_transactions\").alias(\"number_transactions\"), \n",
    "                    f.max(\"number_unique_users\").alias(\"number_unique_users\"), \n",
    "                    f.max(\"capital_turnover\").alias(\"capital_turnover\")\n",
    "                ).collect()\n",
    "        )\n",
    "    \n",
    "    # In case there were no records for that day, return zeroes.\n",
    "    try:\n",
    "        return stats[0].number_transactions, stats[0].number_unique_users, stats[0].capital_turnover\n",
    "    except (IndexError, AttributeError):\n",
    "        if not date: # If there was no record at all.\n",
    "            return 0, 0, 0\n",
    "        return get_bank_statistics()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "Define Cassandra Read Workflow Functions",
     "showTitle": true,
     "inputWidgets": {},
     "nuid": "787419a6-56ed-4416-a8dc-39198ffa4593"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# Get current and previous day dates. As the job will run at 00:00 (the next day), \n",
    "# the current day is actually current day - 1 day.\n",
    "date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "previous_date = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "# date = \"2022-06-07\"\n",
    "# previous_date = \"2022-06-06\"\n",
    "\n",
    "# Get transactions metrics.\n",
    "curr_number_transactions, curr_capital_turnover = get_transactions_metrics(date)\n",
    "# Get users metrics.\n",
    "curr_number_users = get_users_metrics(date)\n",
    "# Get previous day bank statistics.\n",
    "prev_number_transactions, prev_number_users, prev_capital_turnover = get_bank_statistics(previous_date)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "Get Current & Previous Day statistics",
     "showTitle": true,
     "inputWidgets": {},
     "nuid": "28fc00ec-6c6f-4c3a-a1ac-300b37469755"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Spark SQL query for Spark DF.\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    CAST({prev_number_transactions + curr_number_transactions} AS BIGINT) AS number_transactions,\n",
    "    CAST({prev_number_users + curr_number_users} AS BIGINT) AS number_unique_users,\n",
    "    CAST({prev_capital_turnover + curr_capital_turnover} AS BIGINT) AS capital_turnover,\n",
    "    CAST('{date}' AS DATE) AS date\n",
    "\"\"\"\n",
    "\n",
    "# Write the dataframe to Cassandra.\n",
    "spark.sql(query).write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .options(table=OUTPUT_TABLE, keyspace=KEYSPACE) \\\n",
    "        .save()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "Write New Statistics Record to Cassandra",
     "showTitle": true,
     "inputWidgets": {},
     "nuid": "19e19924-119c-42c9-b1cb-ae2065dcf177"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "UNOBANK-STATISTICS",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 388794610728749
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}